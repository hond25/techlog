import os
from dotenv import load_dotenv
import time
import requests
import threading
from bs4 import BeautifulSoup
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, as_completed

from flask import Flask, request, jsonify, g, render_template, redirect, url_for
from flask_cors import CORS
import google.generativeai as genai

import firebase_admin
from firebase_admin import credentials, firestore, auth

import uuid
import google.generativeai as genai

import random

load_dotenv()

try:
    # firebaseã¸ã®æ¥ç¶š
    cred_path = os.environ.get('FIREBASE_ADMINSDK_JSON_PATH')
    cred = credentials.Certificate(cred_path)
    firebase_admin.initialize_app(cred)
    db = firestore.client()
    print("âœ… Firebaseã¨ã®æ¥ç¶šã«æˆåŠŸã—ã¾ã—ãŸã€‚")
except Exception as e:
    print(f"âŒ Firebaseã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    db = None

app = Flask(__name__)
CORS(app)

try:
    # Gemini APIã®ä½¿ç”¨
    API_KEY = os.environ.get('GEMINI_API_KEY')
    if not API_KEY or "YOUR_GEMINI_API_KEY" in API_KEY:
        print("âš ï¸ è­¦å‘Š: Gemini APIã‚­ãƒ¼ãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    genai.configure(api_key=API_KEY)
    model_name = 'gemini-2.5-flash-lite'
    for m in genai.list_models():
        if 'generateContent' in m.supported_generation_methods:
            if 'flash' in m.name:
                model_name = m.name
                break
    
    model = genai.GenerativeModel(model_name)
    print(f"âœ… Geminiãƒ¢ãƒ‡ãƒ« ({model_name}) ã®æº–å‚™ãŒã§ãã¾ã—ãŸã€‚")
except Exception as e:
    print(f"âŒ APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    model = None

# webç”¨ã®ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¼¾ã
def login_required_for_web(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        id_token = request.cookies.get('firebaseToken')
        if not id_token:
            return redirect(url_for('login_page'))
        try:
            decoded_token = auth.verify_id_token(id_token)
            g.user_id = decoded_token['uid']
            g.user = auth.get_user(decoded_token['uid'])
        except Exception as e:
            print(f"Web token verification failed: {e}")
            return redirect(url_for('login_page'))
        return f(*args, **kwargs)
    return decorated_function

# APIç”¨ã®ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¼¾ã
def login_required_for_api(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"error": "Authorization header is missing or invalid"}), 401
        id_token = auth_header.split('Bearer ')[1]
        try:
            decoded_token = auth.verify_id_token(id_token)
            g.user_id = decoded_token['uid']
        except Exception as e:
            print(f"API token verification failed: {e}")
            return jsonify({"error": "Invalid token"}), 401
        return f(*args, **kwargs)
    return decorated_function

# ç¬¬ä¸€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
def is_it_tech(title, url):
    keywords = [
        'æŠ€è¡“', 'IT', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'é–‹ç™º', 'API', 'AI', 'äººå·¥çŸ¥èƒ½', 'Python', 'JavaScript', 
        'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢', 'ã‚¯ãƒ©ã‚¦ãƒ‰', 'ã‚µãƒ¼ãƒ', 'ãƒ‡ãƒ¼ã‚¿', 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'WebæŠ€è¡“', 
        'ITæŠ€è¡“', 'Google', 'Chrome', 'Takeout', 'GitHub', 'ã‚³ãƒ¼ãƒ‰','API', 'AI', 'ML', 'æ©Ÿæ¢°å­¦ç¿’', 
        'Deep Learning', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«', 'åŸºç¤', 'HTML', 'CSS', 'React', 'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰'
    ]
    title = title or ""
    url = url or ""
    for keyword in keywords:
        if keyword.lower() in title.lower() or keyword.lower() in url.lower():
            return True
    return False

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®éš›ã«é™¤å¤–ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
def is_info_page(title, url):
    exclude_keywords = ['ãƒ›ãƒ¼ãƒ ', 'ãƒˆãƒƒãƒ—', 'home', 'drive', 'mail', 'inbox', 'login', 'signin', 'sign in', 'æ¤œç´¢çµæœ','Google cloud','Google æ¤œç´¢',]
    title = title or ""
    url = url or ""
    for kw in exclude_keywords:
        if kw.lower() in title.lower() or kw.lower() in url.lower():
            return False
    if '.google.com' in url and not 'developers.google.com' in url and not 'cloud.google.com/blog' in url:
        return False
    return True

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
def scrape_content(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        ogp_data = {}
        for meta in soup.find_all('meta'):
            prop = meta.get('property')
            if prop and prop.startswith('og:'):
                key = prop.split(':')[1]
                ogp_data[key] = meta.get('content')

        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
            element.decompose()
        text = ' '.join(t.strip() for t in soup.get_text().split())
        return {'text': text[:2000], 'ogp': ogp_data}
    except Exception as e:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return None

# ç¬¬äºŒã®Geminiã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
def classify_content(text_snippet):
    if not model:
        print("ã‚¨ãƒ©ãƒ¼: Geminiãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    prompt = f"""
        ä»¥ä¸‹ã®æ–‡ç« ã¯ã€ŒITæŠ€è¡“è§£èª¬ã®è¨˜äº‹ã€ã‹ã€ŒITç„¡é–¢ä¿‚ã®è¨˜äº‹ã€ã‹ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
        æ–‡ç« : {text_snippet}
        ã€ŒITæŠ€è¡“è§£èª¬ã€ã®å ´åˆã¯ technicalã€ã€ŒITç„¡é–¢ä¿‚ã€ã®å ´åˆã¯ none ã¨ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚
        æƒ…å ±ã‚µã‚¤ãƒˆã®TOPå ´åˆã‚‚noneã¨ç­”ãˆã¦ãã ã•ã„ã€‚
    """
    try:
        response = model.generate_content(prompt)
        answer = response.text.strip().lower()
        return 'technical' in answer
    except Exception as e:
        print(f"Geminiã§ã®åˆ†é¡å¤±æ•—: {e}")
        return False

# Geminiã«ã‚ˆã‚‹è¦ç´„ã‚„ã‚¿ã‚°ä»˜ã®å‡¦ç†
def process_and_summarize_entry(entry):
    """
    å˜ä¸€ã®å±¥æ­´ã‚¨ãƒ³ãƒˆãƒªã«å¯¾ã—ã¦ã€å–å¾—ãƒ»åˆ†é¡ãƒ»è¦ç´„ã¾ã§ã‚’ä¸€è²«ã—ã¦è¡Œã†é–¢æ•°ã€‚
    ThreadPoolExecutorã«ã‚ˆã£ã¦ä¸¦åˆ—ã§å®Ÿè¡Œã•ã‚Œã‚‹ã€‚
    """
    title = entry.get('title', '')
    url = entry.get('url', '')

    if not (url and url.startswith('http') and is_it_tech(title, url) and is_info_page(title, url)):
        return None

    scrape_result = scrape_content(url)
    if not scrape_result:
        return None
    
    content = scrape_result['text']
    ogp_data = scrape_result['ogp']
    if not content or len(content) < 100:
        return None

    if not classify_content(content[:1000]):
        print(f"  -> âŒ æŠ€è¡“è¨˜äº‹ã§ã¯ãªã„ã¨åˆ¤æ–­: {title}")
        return None
    
    print(f"  -> âœ… æŠ€è¡“è¨˜äº‹ã¨ã—ã¦åˆ†é¡: {title}")

    if not model:
        return None
    try:
        prompt = f"""
            ä»¥ä¸‹ã®Webã‚µã‚¤ãƒˆã®å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„:
            ã‚¿ã‚¤ãƒˆãƒ«: {title}
            URL: {url}
            ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {content[:1500]}...
            å¿…ãšä¸‹è¨˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
            ã‚¿ã‚¤ãƒˆãƒ«:ï¼ˆã“ã“ã«AIãŒç”Ÿæˆã—ãŸã€å†…å®¹ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨˜è¼‰ï¼‰
            æƒ…å ±å…ƒ:ï¼ˆã“ã“ã«URLã§ã¯ãªãã€Webã‚µã‚¤ãƒˆåã‚„ã‚µãƒ¼ãƒ“ã‚¹åã‚’è¨˜è¼‰ï¼‰
            è¦ç´„:ï¼ˆã‚µã‚¤ãƒˆå†…å®¹ã‚’ITæŠ€è¡“ã®è¦³ç‚¹ã‹ã‚‰ä¸€è¨€ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚ã‚¿ã‚°ç”¨èªã‚’å¿…ãšå«ã‚ã¦ã€ã§ã‚ã‚‹èª¿ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚å¤ªå­—ãªã©ã¯ãªã—ã§ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§æ›¸ã„ã¦ãã ã•ã„ã€‚ï¼‰
            ã‚¿ã‚°:ï¼ˆé‡è¦ï¼šå¿…ãšä¸‹è¨˜ã®ã€Œã‚¿ã‚°ãƒªã‚¹ãƒˆã€ã®ä¸­ã‹ã‚‰ã€å†…å®¹ã«æœ€ã‚‚é–¢é€£ã™ã‚‹å˜èªã‚’2ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ã€‚ãƒªã‚¹ãƒˆã«ãªã„å˜èªã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ï¼‰
            ã‚¿ã‚°ãƒªã‚¹ãƒˆ: ã‚µãƒ¼ãƒãƒ¼, ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯,HTML, CSS, JavaScript, Java, Python, PHP, Ruby, Rust, swift, Cè¨€èª, C#, C++, TypeScript, Go, ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹, ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹, LLM, Linux, Windows, MacOS, OS,ã‚¯ãƒ©ã‚¦ãƒ‰, AWS, Azure, GCP, Docker, ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯, ãƒ©ã‚¤ãƒ–ãƒ©ãƒª, API, JSON, SQL, NoSQL, Git, Github, AI, UI/UX, Cloud, ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯, ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£, ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰, ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰, Web3
        """
        response = model.generate_content(prompt)
        
        lines = response.text.strip().split('\n')
        article_data = {'originalUrl': url, 'originalTitle': title}
        for line in lines:
            if ':' in line:
                key, value = [x.strip() for x in line.split(':', 1)]
                if key == 'ã‚¿ã‚¤ãƒˆãƒ«': article_data['generatedTitle'] = value
                elif key == 'æƒ…å ±å…ƒ': article_data['source'] = value
                elif key == 'è¦ç´„': article_data['summary'] = value
                elif key == 'ã‚¿ã‚°': article_data['tags'] = [tag.strip() for tag in value.split(',')]

        article_data['ogp'] = ogp_data
        if all(k in article_data for k in ['generatedTitle', 'summary', 'tags']):
            print(f"  -> ğŸ“ è¦ç´„ç”ŸæˆæˆåŠŸ: {article_data.get('generatedTitle')}")
            return article_data
        else:
            print(f"  -> âš ï¸ è¦ç´„çµæœã®å½¢å¼ãŒä¸æ­£: {title}")
            return None

    except Exception as e:
        print(f"  -> ğŸš¨ Geminiã§ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# é‡è¤‡ã—ãŸURLã®é™¤å¤–ã¨ä¸¦åˆ—å‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜
def process_and_summarize_history(history_data, user_id, job_id):
    print(f"\n--- å±¥æ­´ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ (User: {user_id}, Job: {job_id}) ---")
    
    candidate_urls = list(set(entry.get('url') for entry in history_data if entry.get('url')))
    if not candidate_urls:
        print("å‡¦ç†å¯¾è±¡ã®URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        job_ref = db.collection('users').document(user_id).collection('jobs').document(job_id)
        job_ref.update({'status': 'complete', 'completedAt': firestore.SERVER_TIMESTAMP})
        return

    print(f"å±¥æ­´ã‹ã‚‰ {len(candidate_urls)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
    
    existing_urls_in_db = set()
    chunk_size = 30
    for i in range(0, len(candidate_urls), chunk_size):
        chunk = candidate_urls[i:i + chunk_size]
        try:
            query = db.collection('users').document(user_id).collection('articles').where('originalUrl', 'in', chunk)
            docs = query.stream()
            for doc in docs:
                existing_urls_in_db.add(doc.to_dict().get('originalUrl'))
        except Exception as e:
            print(f"URLã®å­˜åœ¨ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    if existing_urls_in_db:
        print(f"DBã«æ—¢ã«å­˜åœ¨ã™ã‚‹URLãŒ {len(existing_urls_in_db)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")

    entries_to_process = [entry for entry in history_data if entry.get('url') not in existing_urls_in_db]
    
    print(f"æ–°è¦å‡¦ç†å¯¾è±¡ã®è¨˜äº‹ã¯ {len(entries_to_process)} ä»¶ã§ã™ã€‚ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    if not entries_to_process:
        print("æ–°è¦å‡¦ç†å¯¾è±¡ã®è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        job_ref = db.collection('users').document(user_id).collection('jobs').document(job_id)
        job_ref.update({'status': 'complete', 'completedAt': firestore.SERVER_TIMESTAMP, 'newArticleIds': []})
        return

    summarized_articles = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_entry = {executor.submit(process_and_summarize_entry, entry): entry for entry in entries_to_process}
        for future in as_completed(future_to_entry):
            result = future.result()
            if result:
                summarized_articles.append(result)

    new_article_ids = []
    if summarized_articles:
        print(f"\n{len(summarized_articles)}ä»¶ã®è¨˜äº‹ã®è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬ä¿å­˜ã—ã¾ã™ã€‚")
        try:
            user_articles_ref = db.collection('users').document(user_id).collection('articles')
            batch = db.batch()
            for article_data in summarized_articles:
                article_data['createdAt'] = firestore.SERVER_TIMESTAMP
                doc_ref = user_articles_ref.document()
                batch.set(doc_ref, article_data)
                new_article_ids.append(doc_ref.id)
            batch.commit()
            print(f"âœ… {len(new_article_ids)}ä»¶ã®è¨˜äº‹ã‚’Firestoreã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ Firestoreã¸ã®ãƒãƒƒãƒä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("è¦ç´„å¯¾è±¡ã®è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    try:
        job_ref = db.collection('users').document(user_id).collection('jobs').document(job_id)
        job_ref.update({
            'status': 'complete',
            'newArticleIds': new_article_ids,
            'completedAt': firestore.SERVER_TIMESTAMP
        })
        print(f"âœ… ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ã¾ã—ãŸ (Job: {job_id})ã€‚æ–°è¦è¨˜äº‹: {len(new_article_ids)}ä»¶")
    except Exception as e:
        print(f"âŒ ã‚¸ãƒ§ãƒ–ã®æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    print("\n--- å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ ---")


@app.route('/')
def index():
    id_token = request.cookies.get('firebaseToken')
    if id_token:
        try:
            auth.verify_id_token(id_token)
            return redirect(url_for('dashboard'))
        except:
            return render_template('login.html')
    return render_template('login.html')

# @app.route('/select')
# @login_required_for_web
# def select_view():
#     user_id = g.user_id
#     try:
#         articles_ref = db.collection('users').document(user_id).collection('articles').stream()
#         all_tags = set()
#         for doc in articles_ref:
#             article_data = doc.to_dict()
#             if 'tags' in article_data and article_data['tags']:
#                 for tag in article_data['tags']:
#                     all_tags.add(tag)
#         sorted_tags = sorted(list(all_tags))
#         return render_template('select_view.html', tags=sorted_tags, user_email=g.user.email)
#     except Exception as e:
#         print(f"âŒ ã‚¿ã‚°ä¸€è¦§ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
#         return "ã‚¿ã‚°ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 500


@app.route('/dashboard')
@login_required_for_web
def dashboard():
    user_id = g.user_id
    tag_filter = request.args.get('filter')
    keyword_query = request.args.get('q')
    search_type = request.args.get('search_type', 'all')

    try:
        recommended_articles = []
        recommended_ids = []
        recommendation_ref = db.collection('users').document(user_id).collection('recommendations').document('weekly')
        recommendation_doc = recommendation_ref.get()

        if recommendation_doc.exists:
            recommended_ids = recommendation_doc.to_dict().get('articleIds', [])
            if recommended_ids:
                articles_ref_for_rec = db.collection('users').document(user_id).collection('articles')
                for article_id in recommended_ids:
                    doc = articles_ref_for_rec.document(article_id).get()
                    if doc.exists:
                        article_data = doc.to_dict()
                        article_data['id'] = doc.id
                        if 'createdAt' in article_data and article_data['createdAt']:
                            article_data['formatted_date'] = article_data['createdAt'].strftime('%Y-%m-%d')
                        recommended_articles.append(article_data)

        articles_ref = db.collection('users').document(user_id).collection('articles')
        
        if tag_filter:
            if tag_filter == 'readLater':
                query = articles_ref.where('readLater', '==', True).order_by('createdAt', direction=firestore.Query.DESCENDING)
            else:
                query = articles_ref.where('tags', 'array_contains', tag_filter).order_by('createdAt', direction=firestore.Query.DESCENDING)
        else:
            query = articles_ref.order_by('createdAt', direction=firestore.Query.DESCENDING)
        
        docs = query.stream()
        
        articles = []
        for doc in docs:
            # if doc.id in recommended_ids:
            #     continue

            article_data = doc.to_dict()
            
            if keyword_query:
                keyword_lower = keyword_query.lower()
                is_match = False

                gen_title = article_data.get('generatedTitle', '').lower()
                orig_title = article_data.get('originalTitle', '').lower()
                summary = article_data.get('summary', '').lower()
                tags = [t.lower() for t in article_data.get('tags', [])]

                reflection_text = ""
                if 'reflection' in article_data and isinstance(article_data['reflection'], dict):
                    r = article_data['reflection']
                    reflection_text += r.get('specific_impression', '').lower()
                    reflection_text += r.get('why_important', '').lower()
                    reflection_text += r.get('what_i_got', '').lower()
                    reflection_text += r.get('memo', '').lower()

                if search_type == 'tag':
                    for tag in tags:
                        if keyword_lower in tag:
                            is_match = True
                            break
                
                elif search_type == 'title':
                    if (keyword_lower in gen_title) or (keyword_lower in orig_title):
                        is_match = True

                elif search_type == 'reflection':
                    if keyword_lower in reflection_text:
                        is_match = True

                else:
                    search_corpus = gen_title + orig_title + summary + reflection_text
                    if keyword_lower in search_corpus:
                        is_match = True
                    else:
                        for tag in tags:
                            if keyword_lower in tag:
                                is_match = True
                                break

                if not is_match:
                    continue

            article_data['id'] = doc.id
            if 'createdAt' in article_data and article_data['createdAt']:
                article_data['formatted_date'] = article_data['createdAt'].strftime('%Y-%m-%d')
            else:
                article_data['formatted_date'] = 'æ—¥ä»˜ãªã—'
            
            articles.append(article_data)

        return render_template('dashboard.html', 
                               user_email=g.user.email, 
                               articles=articles,
                               recommended_articles=recommended_articles,
                               current_filter=tag_filter,
                               keyword_query=keyword_query,
                               search_type=search_type)

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        if 'index' in str(e):
            error_message = """
            <h1>ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼</h1>
            <p>ãƒ‡ãƒ¼ã‚¿ã®çµã‚Šè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã“ã‚Œã¯Firestoreã®ã€Œè¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ãŒä¸è¶³ã—ã¦ã„ã‚‹ã“ã¨ãŒåŸå› ã®å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚</p>
            <p><strong>è§£æ±ºç­–:</strong> ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚„ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹URLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€Firebaseã®ç”»é¢ã®æŒ‡ç¤ºã«å¾“ã£ã¦å¿…è¦ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ä½œæˆã«ã¯æ•°åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚</p>
            """
            return error_message, 500
        return "ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 500
    
@app.route('/article/<article_id>')
@login_required_for_web
def article_detail(article_id):
    user_id = g.user_id
    try:
        doc_ref = db.collection('users').document(user_id).collection('articles').document(article_id)
        doc = doc_ref.get()
        if doc.exists:
            article_data = doc.to_dict()
            article_data['id'] = doc.id
            
            return render_template('article_detail.html', article=article_data, user_email=g.user.email)
        else:
            return "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", 404
    except Exception as e:
        print(f"âŒ è¨˜äº‹è©³ç´°ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return "è¨˜äº‹ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 500
    
@app.route('/article/<article_id>', methods=['DELETE'])
@login_required_for_api
def delete_article(article_id):
    """
    æŒ‡å®šã•ã‚ŒãŸIDã®è¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    """
    user_id = g.user_id
    try:
        doc_ref = db.collection('users').document(user_id).collection('articles').document(article_id)

        if not doc_ref.get().exists:
            return jsonify({"error": "Article not found"}), 404

        doc_ref.delete()
        print(f"âœ… è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸ (User: {user_id}, Article: {article_id})")
        return jsonify({"status": "success", "message": "Article deleted successfully"}), 200
    except Exception as e:
        print(f"âŒ è¨˜äº‹ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to delete article"}), 500

@app.route('/login')
def login_page():
    return render_template('login.html')

@app.route('/history', methods=['POST'])
@login_required_for_api
def receive_history():
    history_data = request.get_json()
    if not isinstance(history_data, list):
        return jsonify({"error": "Invalid JSON"}), 400
    
    user_id = g.user_id
    job_id = str(uuid.uuid4())
    try:
        job_ref = db.collection('users').document(user_id).collection('jobs').document(job_id)
        job_ref.set({
            'status': 'processing',
            'createdAt': firestore.SERVER_TIMESTAMP,
            'newArticleIds': []
        })
        print(f"â¡ï¸  èªè¨¼æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ ({user_id}) ã‹ã‚‰ {len(history_data)}ä»¶ã®å±¥æ­´ã‚’å—ä¿¡ã€‚Job ID: {job_id}")
        
        thread = threading.Thread(target=process_and_summarize_history, args=(history_data, user_id, job_id))
        thread.start()
        
        return jsonify({"status": "processing_started", "job_id": job_id}), 202
    except Exception as e:
        print(f"âŒ ã‚¸ãƒ§ãƒ–ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to create a processing job"}), 500

@app.route('/create_user_profile', methods=['POST'])
@login_required_for_api
def create_user_profile():
    user_id = g.user_id
    data = request.get_json()
    email = data.get('email')
    if not email:
        return jsonify({"error": "Email is required"}), 400
    try:
        user_ref = db.collection('users').document(user_id)
        if not user_ref.get().exists:
            user_ref.set({
                'email': email,
                'createdAt': firestore.SERVER_TIMESTAMP
            })
            print(f"âœ… æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ (User: {user_id})")
        return jsonify({"status": "success"}), 200
    except Exception as e:
        print(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to create user profile"}), 500

@app.route('/article/<article_id>/reflection', methods=['POST'])
@login_required_for_api
def save_reflection(article_id):
    user_id = g.user_id
    data = request.get_json()
    reflection_data = {
        'usefulness' : data.get('usefulness'),
        'impression' : data.get('impression'),
        'specific_impression' : data.get('specific_impression'),
        'why_important' : data.get('why_important'),
        'content_type' : data.get('content_type'),
        'what_i_got' : data.get('what_i_got'),
        'memo' : data.get('memo')
    }
    required_keys = ['usefulness', 'impression', 'content_type', 'specific_impression', 'why_important', 'what_i_got', 'memo']
    if not all(key in data for key in required_keys):
        return jsonify({"error": "Missing reflection data"}), 400
    
    try:
        doc_ref = db.collection('users').document(user_id).collection('articles').document(article_id)
        doc_ref.update({
            'reflection': reflection_data,
            'updatedAt': firestore.SERVER_TIMESTAMP
        })
        print(f"âœ… æŒ¯ã‚Šè¿”ã‚Šã‚’ä¿å­˜ã—ã¾ã—ãŸ (User: {user_id}, Article: {article_id})")
        return jsonify({"status": "success", "message": "Reflection saved successfully"}), 200
    except Exception as e:
        print(f"âŒ æŒ¯ã‚Šè¿”ã‚Šã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to save reflection"}), 500

@app.route('/processing/<job_id>')
@login_required_for_web
def processing_page(job_id):
    return render_template('processing.html', job_id=job_id, user_email=g.user.email)

@app.route('/reflect')
@login_required_for_web
def reflect_page():
    user_id = g.user_id
    ids_str = request.args.get('ids', '')
    index_str = request.args.get('index', '0')
    if not ids_str:
        return redirect(url_for('dashboard'))
    ids = ids_str.split(',')
    index = int(index_str)
    if index >= len(ids):
        return redirect(url_for('dashboard'))
    current_article_id = ids[index]
    try:
        doc_ref = db.collection('users').document(user_id).collection('articles').document(current_article_id)
        max_retries = 3
        doc = None
        for i in range(max_retries):
            doc = doc_ref.get()
            if doc.exists:
                break
            print(f"è¨˜äº‹å–å¾—ãƒªãƒˆãƒ©ã‚¤ä¸­... ({i + 1}/{max_retries}) ID: {current_article_id}")
            time.sleep(0.5)
        if doc and doc.exists:
            article = doc.to_dict()
            progress = f"{index + 1}/{len(ids)}"
            return render_template('reflect.html', 
                                   article=article, 
                                   article_id=current_article_id,
                                   all_ids=ids_str,
                                   current_index=index,
                                   progress=progress,
                                   user_email=g.user.email)
        else:
            print(f"âš ï¸ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (ID: {current_article_id})ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            next_index = index + 1
            if next_index >= len(ids):
                return redirect(url_for('dashboard'))
            return redirect(url_for('reflect_page', ids=ids_str, index=next_index))
    except Exception as e:
        print(f"âŒ æŒ¯ã‚Šè¿”ã‚Šè¨˜äº‹ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return "è¨˜äº‹ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", 500



@app.route('/api/generate-recommendations', methods=['POST'])
@login_required_for_api
def generate_recommendations():
    """
    ã€è‡ªå‹•å®Ÿè¡Œç”¨APIã€‘S/Aãƒ†ã‚£ã‚¢ã€ã¾ãŸã¯ã€Œå¾Œã§è¦‹ã‚‹ã€ãŒè¨­å®šã•ã‚ŒãŸè¨˜äº‹ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã§3ã¤é¸ã³ã€ãŠã™ã™ã‚ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    user_id = g.user_id
    print(f"é€±æ¬¡ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ (User: {user_id})")
    
    try:
        articles_ref = db.collection('users').document(user_id).collection('articles')
        
        query_s = articles_ref.where('reflection.usefulness', '==', 'tier-s').stream()
        query_a = articles_ref.where('reflection.usefulness', '==', 'tier-a').stream()
        
        query_read_later = articles_ref.where('readLater', '==', True).stream()

        candidate_articles = {}
        for doc in query_s:
            candidate_articles[doc.id] = doc
        for doc in query_a:
            candidate_articles[doc.id] = doc
        for doc in query_read_later:
            candidate_articles[doc.id] = doc

        high_value_articles = list(candidate_articles.values())

        if len(high_value_articles) < 3:
            print(f"  -> ãŠã™ã™ã‚å¯¾è±¡ã®è¨˜äº‹ãŒ3ä»¶æœªæº€ã®ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
            return jsonify({"status": "skipped", "reason": "Not enough high-value articles"}), 200

        recommended_docs = random.sample(high_value_articles, 3)
        recommended_ids = [doc.id for doc in recommended_docs]
        
        recommendation_ref = db.collection('users').document(user_id).collection('recommendations').document('weekly')
        recommendation_ref.set({
            'articleIds': recommended_ids,
            'createdAt': firestore.SERVER_TIMESTAMP
        })
        
        print(f"âœ… ãŠã™ã™ã‚è¨˜äº‹ã‚’3ä»¶ä¿å­˜ã—ã¾ã—ãŸ (User: {user_id})")
        return jsonify({"status": "success", "recommended_ids": recommended_ids}), 200

    except Exception as e:
        print(f"âŒ ãŠã™ã™ã‚ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to generate recommendations"}), 500

@app.route('/api/article/<article_id>/read_later', methods=['POST'])
@login_required_for_api
def toggle_read_later(article_id):
    """
    è¨˜äº‹ã®ã€Œå¾Œã§è¦‹ã‚‹ã€çŠ¶æ…‹ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹APIã€‚
    """
    user_id = g.user_id
    try:
        doc_ref = db.collection('users').document(user_id).collection('articles').document(article_id)
        doc = doc_ref.get()

        if not doc.exists:
            return jsonify({"error": "Article not found"}), 404

        current_status = doc.to_dict().get('readLater', False)
        new_status = not current_status
        
        doc_ref.update({'readLater': new_status})
        
        return jsonify({"status": "success", "readLater": new_status}), 200

    except Exception as e:
        print(f"âŒã€Œå¾Œã§è¦‹ã‚‹ã€çŠ¶æ…‹ã®å¤‰æ›´ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({"error": "Failed to update status"}), 500
    
@app.route('/privacy')
def privacy_policy_page():
    """
    ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ãƒšãƒ¼ã‚¸ã€‚
    ã“ã®ãƒšãƒ¼ã‚¸ã¯ãƒ­ã‚°ã‚¤ãƒ³ä¸è¦ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚
    """
    return render_template('privacy.html')
    
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5001))
    
    debug_mode = os.environ.get('FLASK_DEBUG') == '1'
    
    app.run(host='0.0.0.0', port=port, debug=debug_mode)